{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d2fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from torch import Tensor\n",
    "from joblib import dump, load\n",
    "import pandas as pd \n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    " \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes, num_channels=3):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion),)\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class test_CNN:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    #trying on 6 different resnets for different epochs\n",
    "    def resnet18(self, num_classes, channels=3):\n",
    "        return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, channels)\n",
    "\n",
    "    def resnet34(self, num_classes, channels=3):\n",
    "        return ResNet(BasicBlock, [3, 4, 6, 3], num_classes, channels)\n",
    "\n",
    "    def train(self, epoch, model, dataloader, lfn, optimizer):\n",
    "        for i in range(epoch):\n",
    "            model.train()\n",
    "            print(f'Now Entering Epoch {i + 1}')\n",
    "            lss=0.0\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                # send to device\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = lfn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lss += loss.item()\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print('Loss after batch %5d: %.10f' % (batch_idx + 1, lss / 10))\n",
    "                    lss = 0.0\n",
    "        return model\n",
    "\n",
    "    def mainmethod(self, loadname, pre_trained_model = None):\n",
    "        if pre_trained_model == None:\n",
    "            model = self.resnet34(num_classes = 12, channels = 4)\n",
    "        else:\n",
    "            model = pre_trained_model\n",
    "        model = model.to(self.device)\n",
    "\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        read_prep_data = load(loadname)\n",
    "        traindata, trainy = read_prep_data[0], read_prep_data[1]\n",
    "        train_dataloader = DataLoader(dataset=TensorDataset(Tensor(traindata), Tensor(trainy)), batch_size=16, shuffle=True, num_workers=2)\n",
    "        model = self.train(epoch=1, model=model, dataloader=train_dataloader, lfn=loss_function, optimizer=optimizer)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e98e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"CapstoneProject_SubmissionCode_NikhilKorlipara.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/14Vrop0cu_epWj_gQpv1sdADgDwD_-FiE\n",
    "\"\"\"\n",
    "class Submission:\n",
    "    def submit(self, filename, modelname):\n",
    "        outfile = 'Nikhil Korlipara, nk2742.csv'\n",
    "        output_file = open(outfile, 'w')\n",
    "        titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
    "                 'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
    "\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(torch.cuda.is_available())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        test_data = load(filename)\n",
    "        rgb_data = test_data[0]\n",
    "        file_ids = test_data[1]\n",
    "        file_ids = [int(numeric_string) for numeric_string in file_ids]\n",
    "\n",
    "        prediction = np.zeros(shape=(rgb_data.shape[0], 12))\n",
    "        datasetest = TensorDataset(Tensor(rgb_data), Tensor(prediction))\n",
    "        bsz = 16\n",
    "        test_dataloader = DataLoader(dataset=datasetest, batch_size=bsz, shuffle=False, num_workers=2)\n",
    "\n",
    "        model = torch.jit.load(modelname)\n",
    "        model.eval()\n",
    "\n",
    "        for batch_inx, (data, target) in enumerate(test_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            testprediction = model(data)\n",
    "            prediction[batch_inx*bsz:(batch_inx+1)*bsz, :] = testprediction.detach().cpu().numpy()\n",
    "            if batch_inx % 10 == 0:\n",
    "                print('Batch Index: ' + str(batch_inx))\n",
    "\n",
    "        df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame(prediction)], axis=1, names=titles)\n",
    "        df.columns = titles\n",
    "        df.to_csv(outfile, index=False)\n",
    "        print(\"Written to csv file {}\".format(outfile))\n",
    "        print('Complete')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22885e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Now Entering Epoch 1\n",
      "Loss after batch     1: 0.0562697113\n",
      "Loss after batch    11: 0.0599551541\n",
      "Loss after batch    21: 0.0041477865\n",
      "Loss after batch    31: 0.0014663359\n",
      "Loss after batch    41: 0.0006385723\n",
      "Loss after batch    51: 0.0006611973\n",
      "Loss after batch    61: 0.0006403856\n",
      "Loss after batch    71: 0.0004896866\n",
      "Loss after batch    81: 0.0003813461\n",
      "Loss after batch    91: 0.0005204501\n",
      "Loss after batch   101: 0.0003365924\n",
      "Loss after batch   111: 0.0004184877\n",
      "Loss after batch   121: 0.0007139581\n",
      "Loss after batch   131: 0.0005066775\n",
      "Loss after batch   141: 0.0009651308\n",
      "Loss after batch   151: 0.0009558064\n",
      "Loss after batch   161: 0.0006205831\n",
      "Loss after batch   171: 0.0013670005\n",
      "Loss after batch   181: 0.0011568433\n",
      "Loss after batch   191: 0.0007913806\n",
      "Loss after batch   201: 0.0008582915\n",
      "Loss after batch   211: 0.0007084779\n",
      "Now Entering Epoch 2\n",
      "Loss after batch     1: 0.0001368232\n",
      "Loss after batch    11: 0.0012332542\n",
      "Loss after batch    21: 0.0008278305\n",
      "Loss after batch    31: 0.0010081967\n",
      "Loss after batch    41: 0.0006473622\n",
      "Loss after batch    51: 0.0004598269\n",
      "Loss after batch    61: 0.0005651218\n",
      "Loss after batch    71: 0.0004450062\n",
      "Loss after batch    81: 0.0003318360\n",
      "Loss after batch    91: 0.0003228712\n",
      "Loss after batch   101: 0.0002980523\n",
      "Loss after batch   111: 0.0002413582\n",
      "Loss after batch   121: 0.0003748462\n",
      "Loss after batch   131: 0.0004231683\n",
      "Loss after batch   141: 0.0005265905\n",
      "Loss after batch   151: 0.0004773376\n",
      "Loss after batch   161: 0.0005445380\n",
      "Loss after batch   171: 0.0005751505\n",
      "Loss after batch   181: 0.0003970779\n",
      "Loss after batch   191: 0.0004936716\n",
      "Loss after batch   201: 0.0005242805\n",
      "Loss after batch   211: 0.0003972735\n",
      "Now Entering Epoch 3\n",
      "Loss after batch     1: 0.0000744485\n",
      "Loss after batch    11: 0.0006573687\n",
      "Loss after batch    21: 0.0004239981\n",
      "Loss after batch    31: 0.0005095659\n",
      "Loss after batch    41: 0.0005801096\n",
      "Loss after batch    51: 0.0005408498\n",
      "Loss after batch    61: 0.0004338971\n",
      "Loss after batch    71: 0.0007458605\n",
      "Loss after batch    81: 0.0008391774\n",
      "Loss after batch    91: 0.0004421758\n",
      "Loss after batch   101: 0.0005341404\n",
      "Loss after batch   111: 0.0006672427\n",
      "Loss after batch   121: 0.0005310450\n",
      "Loss after batch   131: 0.0007426433\n",
      "Loss after batch   141: 0.0005377129\n",
      "Loss after batch   151: 0.0005518178\n",
      "Loss after batch   161: 0.0006277703\n",
      "Loss after batch   171: 0.0003982935\n",
      "Loss after batch   181: 0.0004644372\n",
      "Loss after batch   191: 0.0004508550\n",
      "Loss after batch   201: 0.0005766337\n",
      "Loss after batch   211: 0.0004109627\n",
      "Now Entering Epoch 4\n",
      "Loss after batch     1: 0.0000494391\n",
      "Loss after batch    11: 0.0009384615\n",
      "Loss after batch    21: 0.0011097613\n",
      "Loss after batch    31: 0.0010384564\n",
      "Loss after batch    41: 0.0010690194\n",
      "Loss after batch    51: 0.0006938309\n",
      "Loss after batch    61: 0.0004971259\n",
      "Loss after batch    71: 0.0004689244\n",
      "Loss after batch    81: 0.0007615934\n",
      "Loss after batch    91: 0.0004405006\n",
      "Loss after batch   101: 0.0005475024\n",
      "Loss after batch   111: 0.0006386625\n",
      "Loss after batch   121: 0.0007062831\n",
      "Loss after batch   131: 0.0005336954\n",
      "Loss after batch   141: 0.0006185338\n",
      "Loss after batch   151: 0.0005639100\n",
      "Loss after batch   161: 0.0004742961\n",
      "Loss after batch   171: 0.0003770658\n",
      "Loss after batch   181: 0.0005097935\n",
      "Loss after batch   191: 0.0003243248\n",
      "Loss after batch   201: 0.0004933021\n",
      "Loss after batch   211: 0.0003479539\n",
      "Now Entering Epoch 5\n",
      "Loss after batch     1: 0.0000622645\n",
      "Loss after batch    11: 0.0004714705\n",
      "Loss after batch    21: 0.0002953366\n",
      "Loss after batch    31: 0.0003420078\n",
      "Loss after batch    41: 0.0004492225\n",
      "Loss after batch    51: 0.0003799641\n",
      "Loss after batch    61: 0.0004010302\n",
      "Loss after batch    71: 0.0003482005\n",
      "Loss after batch    81: 0.0003432857\n",
      "Loss after batch    91: 0.0004380539\n",
      "Loss after batch   101: 0.0003996160\n",
      "Loss after batch   111: 0.0002551162\n",
      "Loss after batch   121: 0.0005795891\n",
      "Loss after batch   131: 0.0004875291\n",
      "Loss after batch   141: 0.0004007027\n",
      "Loss after batch   151: 0.0004269059\n",
      "Loss after batch   161: 0.0003824227\n",
      "Loss after batch   171: 0.0003211486\n",
      "Loss after batch   181: 0.0002563842\n",
      "Loss after batch   191: 0.0003402702\n",
      "Loss after batch   201: 0.0004136735\n",
      "Loss after batch   211: 0.0002673034\n",
      "Now Entering Epoch 6\n",
      "Loss after batch     1: 0.0000488337\n",
      "Loss after batch    11: 0.0005049794\n",
      "Loss after batch    21: 0.0003638279\n",
      "Loss after batch    31: 0.0001907316\n",
      "Loss after batch    41: 0.0002215545\n",
      "Loss after batch    51: 0.0002523937\n",
      "Loss after batch    61: 0.0003195584\n",
      "Loss after batch    71: 0.0002966521\n",
      "Loss after batch    81: 0.0004395336\n",
      "Loss after batch    91: 0.0003944476\n",
      "Loss after batch   101: 0.0003851110\n",
      "Loss after batch   111: 0.0004713174\n",
      "Loss after batch   121: 0.0006820851\n",
      "Loss after batch   131: 0.0009209255\n",
      "Loss after batch   141: 0.0005328160\n",
      "Loss after batch   151: 0.0004880039\n",
      "Loss after batch   161: 0.0004757194\n",
      "Loss after batch   171: 0.0003830121\n",
      "Loss after batch   181: 0.0003195310\n",
      "Loss after batch   191: 0.0007323088\n",
      "Loss after batch   201: 0.0003525901\n",
      "Loss after batch   211: 0.0003678884\n",
      "Now Entering Epoch 7\n",
      "Loss after batch     1: 0.0000272705\n",
      "Loss after batch    11: 0.0002875378\n",
      "Loss after batch    21: 0.0004024567\n",
      "Loss after batch    31: 0.0002787948\n",
      "Loss after batch    41: 0.0003584812\n",
      "Loss after batch    51: 0.0002804935\n",
      "Loss after batch    61: 0.0004221003\n",
      "Loss after batch    71: 0.0003653142\n",
      "Loss after batch    81: 0.0004879701\n",
      "Loss after batch    91: 0.0004203944\n",
      "Loss after batch   101: 0.0003177504\n",
      "Loss after batch   111: 0.0004759151\n",
      "Loss after batch   121: 0.0004312389\n",
      "Loss after batch   131: 0.0003223991\n",
      "Loss after batch   141: 0.0003527780\n",
      "Loss after batch   151: 0.0005112434\n",
      "Loss after batch   161: 0.0008901295\n",
      "Loss after batch   171: 0.0005618167\n",
      "Loss after batch   181: 0.0003362538\n",
      "Loss after batch   191: 0.0003928790\n",
      "Loss after batch   201: 0.0005812272\n",
      "Loss after batch   211: 0.0006277362\n",
      "Now Entering Epoch 8\n",
      "Loss after batch     1: 0.0000519448\n",
      "Loss after batch    11: 0.0004334221\n",
      "Loss after batch    21: 0.0003632125\n",
      "Loss after batch    31: 0.0003134438\n",
      "Loss after batch    41: 0.0005633630\n",
      "Loss after batch    51: 0.0003279468\n",
      "Loss after batch    61: 0.0004124117\n",
      "Loss after batch    71: 0.0004237088\n",
      "Loss after batch    81: 0.0003556321\n",
      "Loss after batch    91: 0.0002005711\n",
      "Loss after batch   101: 0.0002071047\n",
      "Loss after batch   111: 0.0003058290\n",
      "Loss after batch   121: 0.0002544128\n",
      "Loss after batch   131: 0.0003548205\n",
      "Loss after batch   141: 0.0004054701\n",
      "Loss after batch   151: 0.0003235204\n",
      "Loss after batch   161: 0.0003314417\n",
      "Loss after batch   171: 0.0002794732\n",
      "Loss after batch   181: 0.0005711013\n",
      "Loss after batch   191: 0.0002561060\n",
      "Loss after batch   201: 0.0003883187\n",
      "Loss after batch   211: 0.0002792368\n",
      "Now Entering Epoch 9\n",
      "Loss after batch     1: 0.0000427485\n",
      "Loss after batch    11: 0.0003656212\n",
      "Loss after batch    21: 0.0002414113\n",
      "Loss after batch    31: 0.0002716124\n",
      "Loss after batch    41: 0.0004494871\n",
      "Loss after batch    51: 0.0003183045\n",
      "Loss after batch    61: 0.0003743625\n",
      "Loss after batch    71: 0.0003955697\n",
      "Loss after batch    81: 0.0004062473\n",
      "Loss after batch    91: 0.0004658554\n",
      "Loss after batch   101: 0.0004478665\n",
      "Loss after batch   111: 0.0004132551\n",
      "Loss after batch   121: 0.0004013257\n",
      "Loss after batch   131: 0.0003142249\n",
      "Loss after batch   141: 0.0003237601\n",
      "Loss after batch   151: 0.0003296239\n",
      "Loss after batch   161: 0.0003745481\n",
      "Loss after batch   171: 0.0004863003\n",
      "Loss after batch   181: 0.0004305576\n",
      "Loss after batch   191: 0.0003498382\n",
      "Loss after batch   201: 0.0002976456\n",
      "Loss after batch   211: 0.0003505234\n",
      "Now Entering Epoch 10\n",
      "Loss after batch     1: 0.0001252106\n",
      "Loss after batch    11: 0.0009276023\n",
      "Loss after batch    21: 0.0006943375\n",
      "Loss after batch    31: 0.0004246753\n",
      "Loss after batch    41: 0.0003219621\n",
      "Loss after batch    51: 0.0002805149\n",
      "Loss after batch    61: 0.0002242074\n",
      "Loss after batch    71: 0.0003315041\n",
      "Loss after batch    81: 0.0003940404\n",
      "Loss after batch    91: 0.0004537850\n",
      "Loss after batch   101: 0.0002597493\n",
      "Loss after batch   111: 0.0001653039\n",
      "Loss after batch   121: 0.0001943059\n",
      "Loss after batch   131: 0.0002352804\n",
      "Loss after batch   141: 0.0002414964\n",
      "Loss after batch   151: 0.0002767001\n",
      "Loss after batch   161: 0.0003516379\n",
      "Loss after batch   171: 0.0003308014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch   181: 0.0003402178\n",
      "Loss after batch   191: 0.0002093557\n",
      "Loss after batch   201: 0.0002014052\n",
      "Loss after batch   211: 0.0002104756\n",
      "Now Entering Epoch 11\n",
      "Loss after batch     1: 0.0000316362\n",
      "Loss after batch    11: 0.0002392519\n",
      "Loss after batch    21: 0.0001037627\n",
      "Loss after batch    31: 0.0002266235\n",
      "Loss after batch    41: 0.0002695191\n",
      "Loss after batch    51: 0.0003138396\n",
      "Loss after batch    61: 0.0003043489\n",
      "Loss after batch    71: 0.0002584174\n",
      "Loss after batch    81: 0.0003048159\n",
      "Loss after batch    91: 0.0003390186\n",
      "Loss after batch   101: 0.0004314284\n",
      "Loss after batch   111: 0.0003903114\n",
      "Loss after batch   121: 0.0002323571\n",
      "Loss after batch   131: 0.0002680709\n",
      "Loss after batch   141: 0.0002876039\n",
      "Loss after batch   151: 0.0001784844\n",
      "Loss after batch   161: 0.0001773414\n",
      "Loss after batch   171: 0.0003629697\n",
      "Loss after batch   181: 0.0002520950\n",
      "Loss after batch   191: 0.0002722403\n",
      "Loss after batch   201: 0.0005054974\n",
      "Loss after batch   211: 0.0004660388\n",
      "Now Entering Epoch 1\n",
      "Loss after batch     1: 0.0007457983\n",
      "Loss after batch    11: 0.0865250408\n",
      "Loss after batch    21: 0.0025259017\n",
      "Loss after batch    31: 0.0011633234\n",
      "Loss after batch    41: 0.0006373734\n",
      "Loss after batch    51: 0.0004692654\n",
      "Loss after batch    61: 0.0004298398\n",
      "Loss after batch    71: 0.0002978061\n",
      "Loss after batch    81: 0.0002751635\n",
      "Loss after batch    91: 0.0004226401\n",
      "Loss after batch   101: 0.0003705934\n",
      "Loss after batch   111: 0.0003452643\n",
      "Loss after batch   121: 0.0002634365\n",
      "Loss after batch   131: 0.0002577086\n",
      "Loss after batch   141: 0.0002089719\n",
      "Loss after batch   151: 0.0001680464\n",
      "Loss after batch   161: 0.0002106130\n",
      "Loss after batch   171: 0.0001579263\n",
      "Loss after batch   181: 0.0002465881\n",
      "Loss after batch   191: 0.0003453235\n",
      "Loss after batch   201: 0.0003106835\n",
      "Loss after batch   211: 0.0002595859\n",
      "Now Entering Epoch 2\n",
      "Loss after batch     1: 0.0000098686\n",
      "Loss after batch    11: 0.0002051009\n",
      "Loss after batch    21: 0.0001790029\n",
      "Loss after batch    31: 0.0001761969\n",
      "Loss after batch    41: 0.0002032382\n",
      "Loss after batch    51: 0.0002386597\n",
      "Loss after batch    61: 0.0002870419\n",
      "Loss after batch    71: 0.0003477297\n",
      "Loss after batch    81: 0.0002244673\n",
      "Loss after batch    91: 0.0003401330\n",
      "Loss after batch   101: 0.0001940943\n",
      "Loss after batch   111: 0.0001459621\n",
      "Loss after batch   121: 0.0001739272\n",
      "Loss after batch   131: 0.0002602521\n",
      "Loss after batch   141: 0.0001781104\n",
      "Loss after batch   151: 0.0001742861\n",
      "Loss after batch   161: 0.0002551445\n",
      "Loss after batch   171: 0.0002055066\n",
      "Loss after batch   181: 0.0003417479\n",
      "Loss after batch   191: 0.0002975895\n",
      "Loss after batch   201: 0.0003015212\n",
      "Loss after batch   211: 0.0002262983\n",
      "Now Entering Epoch 3\n",
      "Loss after batch     1: 0.0000428685\n",
      "Loss after batch    11: 0.0002546869\n",
      "Loss after batch    21: 0.0002447264\n",
      "Loss after batch    31: 0.0003157776\n",
      "Loss after batch    41: 0.0002704724\n",
      "Loss after batch    51: 0.0002286755\n",
      "Loss after batch    61: 0.0002192932\n",
      "Loss after batch    71: 0.0002792514\n",
      "Loss after batch    81: 0.0004825073\n",
      "Loss after batch    91: 0.0003777797\n",
      "Loss after batch   101: 0.0002408160\n",
      "Loss after batch   111: 0.0002720980\n",
      "Loss after batch   121: 0.0002295953\n",
      "Loss after batch   131: 0.0003461458\n",
      "Loss after batch   141: 0.0002217234\n",
      "Loss after batch   151: 0.0002137509\n",
      "Loss after batch   161: 0.0001791357\n",
      "Loss after batch   171: 0.0001639436\n",
      "Loss after batch   181: 0.0001950108\n",
      "Loss after batch   191: 0.0002089672\n",
      "Loss after batch   201: 0.0002327005\n",
      "Loss after batch   211: 0.0001792266\n",
      "Now Entering Epoch 4\n",
      "Loss after batch     1: 0.0000314778\n",
      "Loss after batch    11: 0.0002040530\n",
      "Loss after batch    21: 0.0002087139\n",
      "Loss after batch    31: 0.0003282498\n",
      "Loss after batch    41: 0.0003396234\n",
      "Loss after batch    51: 0.0007347965\n",
      "Loss after batch    61: 0.0003987271\n",
      "Loss after batch    71: 0.0004839943\n",
      "Loss after batch    81: 0.0004384777\n",
      "Loss after batch    91: 0.0002342016\n",
      "Loss after batch   101: 0.0002453249\n",
      "Loss after batch   111: 0.0002830761\n",
      "Loss after batch   121: 0.0002242737\n",
      "Loss after batch   131: 0.0002388620\n",
      "Loss after batch   141: 0.0002829702\n",
      "Loss after batch   151: 0.0002183574\n",
      "Loss after batch   161: 0.0001828439\n",
      "Loss after batch   171: 0.0002224297\n",
      "Loss after batch   181: 0.0003047977\n",
      "Loss after batch   191: 0.0002743444\n",
      "Loss after batch   201: 0.0002469423\n",
      "Loss after batch   211: 0.0003273505\n",
      "Now Entering Epoch 5\n",
      "Loss after batch     1: 0.0000333791\n",
      "Loss after batch    11: 0.0002270688\n",
      "Loss after batch    21: 0.0002669401\n",
      "Loss after batch    31: 0.0002652810\n",
      "Loss after batch    41: 0.0002692089\n",
      "Loss after batch    51: 0.0002814187\n",
      "Loss after batch    61: 0.0003065248\n",
      "Loss after batch    71: 0.0003207611\n",
      "Loss after batch    81: 0.0003434970\n",
      "Loss after batch    91: 0.0002379619\n",
      "Loss after batch   101: 0.0002065515\n",
      "Loss after batch   111: 0.0002960142\n",
      "Loss after batch   121: 0.0002269739\n",
      "Loss after batch   131: 0.0002251443\n",
      "Loss after batch   141: 0.0002560146\n",
      "Loss after batch   151: 0.0001732961\n",
      "Loss after batch   161: 0.0002784836\n",
      "Loss after batch   171: 0.0003455042\n",
      "Loss after batch   181: 0.0003198908\n",
      "Loss after batch   191: 0.0003580773\n",
      "Loss after batch   201: 0.0003115377\n",
      "Loss after batch   211: 0.0002154107\n",
      "Now Entering Epoch 6\n",
      "Loss after batch     1: 0.0000139420\n",
      "Loss after batch    11: 0.0003689115\n",
      "Loss after batch    21: 0.0004134758\n",
      "Loss after batch    31: 0.0002453464\n",
      "Loss after batch    41: 0.0004022064\n",
      "Loss after batch    51: 0.0003732398\n",
      "Loss after batch    61: 0.0002803196\n",
      "Loss after batch    71: 0.0002483045\n",
      "Loss after batch    81: 0.0003291249\n",
      "Loss after batch    91: 0.0003284263\n",
      "Loss after batch   101: 0.0002361825\n",
      "Loss after batch   111: 0.0001507894\n",
      "Loss after batch   121: 0.0001950147\n",
      "Loss after batch   131: 0.0002312882\n",
      "Loss after batch   141: 0.0002523591\n",
      "Loss after batch   151: 0.0002043347\n",
      "Loss after batch   161: 0.0002051621\n",
      "Loss after batch   171: 0.0001420777\n",
      "Loss after batch   181: 0.0001672683\n",
      "Loss after batch   191: 0.0002228371\n",
      "Loss after batch   201: 0.0002275557\n",
      "Loss after batch   211: 0.0002066634\n",
      "Now Entering Epoch 7\n",
      "Loss after batch     1: 0.0000414070\n",
      "Loss after batch    11: 0.0003088797\n",
      "Loss after batch    21: 0.0002842856\n",
      "Loss after batch    31: 0.0003197250\n",
      "Loss after batch    41: 0.0001968191\n",
      "Loss after batch    51: 0.0001838237\n",
      "Loss after batch    61: 0.0001863965\n",
      "Loss after batch    71: 0.0001493604\n",
      "Loss after batch    81: 0.0001874828\n",
      "Loss after batch    91: 0.0001565243\n",
      "Loss after batch   101: 0.0002097784\n",
      "Loss after batch   111: 0.0002769933\n",
      "Loss after batch   121: 0.0002391016\n",
      "Loss after batch   131: 0.0002152648\n",
      "Loss after batch   141: 0.0002245400\n",
      "Loss after batch   151: 0.0002014275\n",
      "Loss after batch   161: 0.0001256798\n",
      "Loss after batch   171: 0.0001344758\n",
      "Loss after batch   181: 0.0001231009\n",
      "Loss after batch   191: 0.0001604622\n",
      "Loss after batch   201: 0.0002155750\n",
      "Loss after batch   211: 0.0002200674\n",
      "Now Entering Epoch 8\n",
      "Loss after batch     1: 0.0000302213\n",
      "Loss after batch    11: 0.0001886881\n",
      "Loss after batch    21: 0.0001497633\n",
      "Loss after batch    31: 0.0001795677\n",
      "Loss after batch    41: 0.0002103437\n",
      "Loss after batch    51: 0.0001366170\n",
      "Loss after batch    61: 0.0001644928\n",
      "Loss after batch    71: 0.0002098348\n",
      "Loss after batch    81: 0.0002107736\n",
      "Loss after batch    91: 0.0001463378\n",
      "Loss after batch   101: 0.0001085071\n",
      "Loss after batch   111: 0.0001430178\n",
      "Loss after batch   121: 0.0001929541\n",
      "Loss after batch   131: 0.0001886689\n",
      "Loss after batch   141: 0.0001919817\n",
      "Loss after batch   151: 0.0001609639\n",
      "Loss after batch   161: 0.0001678258\n",
      "Loss after batch   171: 0.0002878375\n",
      "Loss after batch   181: 0.0002478940\n",
      "Loss after batch   191: 0.0001491903\n",
      "Loss after batch   201: 0.0001553252\n",
      "Loss after batch   211: 0.0001975204\n",
      "Now Entering Epoch 9\n",
      "Loss after batch     1: 0.0000354420\n",
      "Loss after batch    11: 0.0001665188\n",
      "Loss after batch    21: 0.0001317500\n",
      "Loss after batch    31: 0.0001338529\n",
      "Loss after batch    41: 0.0002034658\n",
      "Loss after batch    51: 0.0002308760\n",
      "Loss after batch    61: 0.0001995007\n",
      "Loss after batch    71: 0.0003026195\n",
      "Loss after batch    81: 0.0002407841\n",
      "Loss after batch    91: 0.0002002308\n",
      "Loss after batch   101: 0.0002258819\n",
      "Loss after batch   111: 0.0002412491\n",
      "Loss after batch   121: 0.0002012683\n",
      "Loss after batch   131: 0.0001403228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch   141: 0.0001483192\n",
      "Loss after batch   151: 0.0001912497\n",
      "Loss after batch   161: 0.0002141182\n",
      "Loss after batch   171: 0.0002907523\n",
      "Loss after batch   181: 0.0002463141\n",
      "Loss after batch   191: 0.0002405119\n",
      "Loss after batch   201: 0.0001543956\n",
      "Loss after batch   211: 0.0002043064\n",
      "Now Entering Epoch 10\n",
      "Loss after batch     1: 0.0000319853\n",
      "Loss after batch    11: 0.0003146231\n",
      "Loss after batch    21: 0.0002589160\n",
      "Loss after batch    31: 0.0001882811\n",
      "Loss after batch    41: 0.0001596248\n",
      "Loss after batch    51: 0.0001758260\n",
      "Loss after batch    61: 0.0001367337\n",
      "Loss after batch    71: 0.0001567773\n",
      "Loss after batch    81: 0.0001449529\n",
      "Loss after batch    91: 0.0001594667\n",
      "Loss after batch   101: 0.0001230834\n",
      "Loss after batch   111: 0.0001036623\n",
      "Loss after batch   121: 0.0001875462\n",
      "Loss after batch   131: 0.0001794502\n",
      "Loss after batch   141: 0.0001761312\n",
      "Loss after batch   151: 0.0002020977\n",
      "Loss after batch   161: 0.0002267340\n",
      "Loss after batch   171: 0.0002909707\n",
      "Loss after batch   181: 0.0002168259\n",
      "Loss after batch   191: 0.0002387385\n",
      "Loss after batch   201: 0.0001941398\n",
      "Loss after batch   211: 0.0002583201\n",
      "Now Entering Epoch 11\n",
      "Loss after batch     1: 0.0000221105\n",
      "Loss after batch    11: 0.0001914067\n",
      "Loss after batch    21: 0.0001875128\n",
      "Loss after batch    31: 0.0001409612\n",
      "Loss after batch    41: 0.0001909330\n",
      "Loss after batch    51: 0.0002184207\n",
      "Loss after batch    61: 0.0002457834\n",
      "Loss after batch    71: 0.0001519194\n",
      "Loss after batch    81: 0.0001686757\n",
      "Loss after batch    91: 0.0001962066\n",
      "Loss after batch   101: 0.0001910450\n",
      "Loss after batch   111: 0.0002155747\n",
      "Loss after batch   121: 0.0002058979\n",
      "Loss after batch   131: 0.0002211523\n",
      "Loss after batch   141: 0.0001774307\n",
      "Loss after batch   151: 0.0002168683\n",
      "Loss after batch   161: 0.0003878542\n",
      "Loss after batch   171: 0.0002431584\n",
      "Loss after batch   181: 0.0002319358\n",
      "Loss after batch   191: 0.0002027208\n",
      "Loss after batch   201: 0.0002267402\n",
      "Loss after batch   211: 0.0001753841\n",
      "Now Entering Epoch 1\n",
      "Loss after batch     1: 0.0001566856\n",
      "Loss after batch    11: 0.0479856165\n",
      "Loss after batch    21: 0.0016513128\n",
      "Loss after batch    31: 0.0007697376\n",
      "Loss after batch    41: 0.0005259726\n",
      "Loss after batch    51: 0.0005128432\n",
      "Loss after batch    61: 0.0003926943\n",
      "Loss after batch    71: 0.0003505804\n",
      "Loss after batch    81: 0.0002608743\n",
      "Loss after batch    91: 0.0002945421\n",
      "Loss after batch   101: 0.0003313836\n",
      "Loss after batch   111: 0.0002455698\n",
      "Loss after batch   121: 0.0002889936\n",
      "Loss after batch   131: 0.0003326354\n",
      "Loss after batch   141: 0.0002292914\n",
      "Loss after batch   151: 0.0003417180\n",
      "Loss after batch   161: 0.0002895973\n",
      "Loss after batch   171: 0.0002822568\n",
      "Loss after batch   181: 0.0002919550\n",
      "Loss after batch   191: 0.0002860206\n",
      "Loss after batch   201: 0.0002093395\n",
      "Loss after batch   211: 0.0001979200\n",
      "Now Entering Epoch 2\n",
      "Loss after batch     1: 0.0000528084\n",
      "Loss after batch    11: 0.0002518623\n",
      "Loss after batch    21: 0.0002460625\n",
      "Loss after batch    31: 0.0003005934\n",
      "Loss after batch    41: 0.0003470201\n",
      "Loss after batch    51: 0.0003225998\n",
      "Loss after batch    61: 0.0002883549\n",
      "Loss after batch    71: 0.0003278142\n",
      "Loss after batch    81: 0.0002986498\n",
      "Loss after batch    91: 0.0002042756\n",
      "Loss after batch   101: 0.0001491517\n",
      "Loss after batch   111: 0.0002218339\n",
      "Loss after batch   121: 0.0001776933\n",
      "Loss after batch   131: 0.0001877437\n",
      "Loss after batch   141: 0.0002256910\n",
      "Loss after batch   151: 0.0001848202\n",
      "Loss after batch   161: 0.0001770662\n",
      "Loss after batch   171: 0.0001948126\n",
      "Loss after batch   181: 0.0001582166\n",
      "Loss after batch   191: 0.0001103926\n",
      "Loss after batch   201: 0.0002030486\n",
      "Loss after batch   211: 0.0001769064\n",
      "Now Entering Epoch 3\n",
      "Loss after batch     1: 0.0000527047\n",
      "Loss after batch    11: 0.0002272474\n",
      "Loss after batch    21: 0.0002542068\n",
      "Loss after batch    31: 0.0002817955\n",
      "Loss after batch    41: 0.0002148354\n",
      "Loss after batch    51: 0.0003018032\n",
      "Loss after batch    61: 0.0002272599\n",
      "Loss after batch    71: 0.0001739649\n",
      "Loss after batch    81: 0.0001722588\n",
      "Loss after batch    91: 0.0001593065\n",
      "Loss after batch   101: 0.0002428119\n",
      "Loss after batch   111: 0.0002031668\n",
      "Loss after batch   121: 0.0001270263\n",
      "Loss after batch   131: 0.0002252450\n",
      "Loss after batch   141: 0.0002892884\n",
      "Loss after batch   151: 0.0001834636\n",
      "Loss after batch   161: 0.0001697210\n",
      "Loss after batch   171: 0.0002253737\n",
      "Loss after batch   181: 0.0001881629\n",
      "Loss after batch   191: 0.0001626200\n",
      "Loss after batch   201: 0.0001204462\n",
      "Loss after batch   211: 0.0001346667\n",
      "Now Entering Epoch 4\n",
      "Loss after batch     1: 0.0000461071\n",
      "Loss after batch    11: 0.0001086950\n",
      "Loss after batch    21: 0.0001236921\n",
      "Loss after batch    31: 0.0001169537\n",
      "Loss after batch    41: 0.0000996911\n",
      "Loss after batch    51: 0.0001023335\n",
      "Loss after batch    61: 0.0001281550\n",
      "Loss after batch    71: 0.0001670515\n",
      "Loss after batch    81: 0.0001409516\n",
      "Loss after batch    91: 0.0001225906\n",
      "Loss after batch   101: 0.0001233254\n",
      "Loss after batch   111: 0.0001674583\n",
      "Loss after batch   121: 0.0001128847\n",
      "Loss after batch   131: 0.0001482196\n",
      "Loss after batch   141: 0.0001520603\n",
      "Loss after batch   151: 0.0001116857\n",
      "Loss after batch   161: 0.0001407113\n",
      "Loss after batch   171: 0.0001258672\n",
      "Loss after batch   181: 0.0002025963\n",
      "Loss after batch   191: 0.0001530044\n",
      "Loss after batch   201: 0.0001611297\n",
      "Loss after batch   211: 0.0001868908\n",
      "Now Entering Epoch 5\n",
      "Loss after batch     1: 0.0000216177\n",
      "Loss after batch    11: 0.0001460326\n",
      "Loss after batch    21: 0.0002324536\n",
      "Loss after batch    31: 0.0002813109\n",
      "Loss after batch    41: 0.0003293702\n",
      "Loss after batch    51: 0.0003684552\n",
      "Loss after batch    61: 0.0002755118\n",
      "Loss after batch    71: 0.0002494261\n",
      "Loss after batch    81: 0.0001650541\n",
      "Loss after batch    91: 0.0001850000\n",
      "Loss after batch   101: 0.0001293759\n",
      "Loss after batch   111: 0.0001175861\n",
      "Loss after batch   121: 0.0001832272\n",
      "Loss after batch   131: 0.0001796126\n",
      "Loss after batch   141: 0.0001478015\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "cnn_model = test_CNN()\n",
    "cnn_model_0 = cnn_model.mainmethod(loadname = 'preprocessed_train_data0.joblib', pre_trained_model = None)\n",
    "model_scripted = torch.jit.script(cnn_model_0) # Export to TorchScript\n",
    "model_scripted.save('res34_pretrained_model_0.pt')\n",
    "\n",
    "#cnn_model_1 = cnn_model.mainmethod(loadname = 'preprocessed_train_data1.joblib', pre_trained_model = None)\n",
    "#model_scripted = torch.jit.script(cnn_model_1) # Export to TorchScript\n",
    "#model_scripted.save('res34_pretrained_model_0.pt')\n",
    "\n",
    "#cnn_model_2 = cnn_model.mainmethod(loadname = 'preprocessed_train_data2.joblib', pre_trained_model = None)\n",
    "#model_scripted = torch.jit.script(cnn_model_2) # Export to TorchScript\n",
    "#model_scripted.save('res34_pretrained_model_2.pt')\n",
    "\n",
    "cnn_model_combined = cnn_model.mainmethod(loadname = 'preprocessed_train_data1.joblib', pre_trained_model = cnn_model_0)\n",
    "cnn_model_combined = cnn_model.mainmethod(loadname = 'preprocessed_train_data2.joblib', pre_trained_model = cnn_model_combined)\n",
    "\n",
    "model_scripted = torch.jit.script(cnn_model_combined) # Export to TorchScript\n",
    "model_scripted.save('res34_pretrained_combined_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6724f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Batch Index: 0\n",
      "Batch Index: 10\n",
      "Batch Index: 20\n",
      "Batch Index: 30\n",
      "Batch Index: 40\n",
      "Batch Index: 50\n",
      "Written to csv file Nikhil Korlipara, nk2742.csv\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "sub = Submission()\n",
    "df = sub.submit(filename = 'preprocessed_test_img0.joblib', modelname = 'res34_pretrained_model_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210d36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
